---
layout: archive
title: "Research Interests & Projects"
permalink: /research/
author_profile: true
---

*This page is under construction*

### [Current Projects](#current-projects) &nbsp;&nbsp;&nbsp; [Previous Projects](#previous-projects)  &nbsp;&nbsp;&nbsp; [Collaborators](#collaborators) 

Current Projects
------

1. **<font color='#b300b3'>Speech Activity Detection for Prompted Pre-School Children Speech</font>**: Coming soon !


2. **<font color='#009933'>Sequential Pattern Learning in children with Developmental Language Disorder</font>**: Coming soon!

Previous Projects
------

1. **<font color='#994d00'>Articulatory Kinematic Analysis of subjects with Amyotrophic Lateral Sclerosis & Laryngectomy</font>** (PI: Dr. Jun Wang): Coming soon!

1. **<font color='#994d00'>Articulatory Kinematic Analysis of subjects with Laryngectomy vs. Silent Speech</font>** (PI: Dr. Jun Wang): Laryngectomy is the surgical removal of a larynx due to oral or laryngeal cancer. After the larynx is removed, the articulatory control system cannot modify the length of the vocal tract based on larynx height and the patients lose the ability to control pitch. To overcome limitations of available voice prosthesis options, studies have focused on the development of silent speech interface (SSI) technology which allows for communication without the vibration of the vocal folds. Specifically, SSIs convert articulatory motion data to an acoustic output. A better understanding of alaryngeal speech is needed to advance scientific knowledge of motor control and for the development of SSI technology. The aim of this study was to characterize articulatory movements during the production of phrases obtained from healthy (voiced, silent) speakers and laryngectomees. In addition, a support vector machine (SVM) model was used to classify the three speech modes based on tongue and lip motion patterns.  \[[Poster](https://satwikdutta.github.io/files/2020_MotorSpeech.pdf)\]

1. **<font color='#994d00'>Real-Time Voice Activity Detection from Non-Invasive Neuromagnetic Signals</font>** (PI: Dr. Jun Wang): Neural speech decoding-driven brain-computer interface (BCI) or speech-BCI is a novel paradigm for exploring communication restoration for locked-in (fully paralyzed but aware) patients. Speech-BCIs aim to map a direct transformation from neural signals to text or speech, which has the potential for a higher communication rate than the current BCIs. Although recent progress has demonstrated the potential of speech-BCIs from either invasive or non-invasive neural signals, the majority of the systems developed so far still assume knowing the onset and offset of the speech utterances within the continuous neural recordings. To address this issue, in this study, an attempt was made to automatically detect the voice/speech activity directly from the neural signals recorded using magnetoencephalography (MEG) using long short-term memory-recurrent neural networks (LSTM-RNN). \[[Journal](https://doi.org/10.3390/s20082248)\]


Collaborators
------

* Dr. John H.L. Hansen, University of Texas at Dallas, USA.
* Dr. Lisa Goffman, University of Texas at Dallas, USA.
* Dr. Thomas F. Campbell, University of Texas at Dallas, USA.
* Dr. Jun Wang, University of Texas at Austin, USA.
* Dr. Sebastian Helie, Purdue University, USA. 
* Dr. Christine A. Dollaghan, University of Texas at Dallas, USA. 
* Dr. Johanna Rudolph, University of Texas at Dallas, USA. 
* Dr. Alan Wisler, University of Texas at Austin, USA.
* Prasanna V. Kothalkar, University of Texas at Dallas, USA.
* Kathryn Kreidler, University of Texas at Dallas, USA.
* Leah Sack, University of Texas at Dallas, USA.
* Kristin Teplansky, University of Texas at Austin, USA.
* Debadatta Dash, University of Texas at Austin, USA.
